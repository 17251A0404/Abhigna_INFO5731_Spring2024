{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17251A0404/Abhigna_INFO5731_Spring2024/blob/main/DARA_ABHIGNA_Inclass_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "41fc2332-e25d-40ff-f79d-ed13e2741fcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Identifying Fake News in Online Articles is an intriguing text classification topic that we may discuss. In the current digital era, when the propagation of fake news may have serious repercussions, identifying disinformation is essential. The goal of this work is to develop a machine learning model that can discriminate between authentic and fraudulent news sources.\\nThe features:\\n\\nSource Credibility Score:  An indicator of the news source's reliability.\\nexplanation: Reputable sources are typically the source of high-quality journalism. Evaluating a source's historical credibility may help a model by taking into account things like public trust, journalistic honors, and historical fact-checking. These factors can be used to generate a weighted score.\\nEmotive Tone Analysis: The text's emotional tone is identified.\\nIn order to get attention, fake news frequently preys on emotions. Features that analyze the article's overall emotional tone, identify emotional triggers, and perform sentiment analysis might be useful. False information might be linked to inflated happy or negative emotions.\\nSemantic Analysis of Named Entities:Semantic analysis involves extracting and analyzing named entities (people, places, and organizations) from an article.Fake news might entail the abuse or fabrication of information about relevant entities. The analysis of named entities can show patterns of misrepresentation. For example, spotting contradictory references of persons, locations, or organizations throughout the article and cross-referencing with credible databases.\\nContextual Coherence Score: A score indicating the coherence and consistency of the material in the article.\\nExplanation: Misleading articles may lack logical coherence or include contradicting information. Features such as coherence analysis, testing for logical fallacies, and verifying that information is consistent with known facts can all help the model uncover disinformation.\\nStance Analysis involves evaluating an article's perspective on a certain issue or event.\\n Fake news frequently uses a biased or extreme viewpoint to influence public opinion. Stance analysis entails comprehending the perspective offered in the text. Loaded language, biased framing, and the existence of opinionated assertions are all possible features.\\n\\nIn summary, the qualities mentioned above encompass a wide range of elements, including source credibility, emotional tone, semantic consistency, contextual coherence, and the overall position of the piece. By combining these many elements, the machine learning model may get a thorough grasp of the content, hence boosting its capacity to distinguish between authentic and fraudulent news.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "One interesting text classification task could be sentiment analysis on social media comments regarding a particular product or service. This task involves determining the sentiment (positive, negative, or neutral) expressed in each comment. Here are five types of features that could be useful for building a machine learning model for sentiment analysis:\n",
        "\n",
        "1. **Bag-of-Words (BoW)**:\n",
        "   - This feature represents the frequency of each word in the text.\n",
        "   - BoW features capture the presence or absence of specific words, which can indicate sentiment polarity.\n",
        "   - Words like \"great,\" \"awesome,\" and \"love\" are likely indicators of positive sentiment, while words like \"awful,\" \"disappointed,\" and \"hate\" may indicate negative sentiment.\n",
        "\n",
        "2. **N-grams**:\n",
        "   - N-grams are sequences of adjacent words in the text.\n",
        "   - Bi-grams (sequences of two words) and tri-grams (sequences of three words) can capture contextual information better than individual words.\n",
        "   - For instance, the phrase \"not good\" would be represented as a bi-gram, which is crucial for capturing negation in sentiment analysis.\n",
        "\n",
        "3. **Word Embeddings**:\n",
        "   - Word embeddings represent words as dense vectors in a continuous vector space.\n",
        "   - Pre-trained word embeddings like Word2Vec, GloVe, or fastText can capture semantic relationships between words.\n",
        "   - These embeddings encode contextual and semantic information, which can enhance the model's understanding of sentiment nuances.\n",
        "\n",
        "4. **Part-of-Speech (POS) Tags**:\n",
        "   - POS tags represent the grammatical category of each word in the text (e.g., noun, verb, adjective).\n",
        "   - Adjectives and adverbs often carry sentiment information. For example, positive sentiments are often expressed through adjectives like \"amazing\" or \"beautiful.\"\n",
        "   - Incorporating POS tags as features can help the model focus on the most informative words for sentiment analysis.\n",
        "\n",
        "5. **Sentiment Lexicons**:\n",
        "   - Sentiment lexicons contain lists of words labeled with their associated sentiment polarity (positive, negative, or neutral).\n",
        "   - Features based on sentiment lexicons can capture sentiment signals directly without needing extensive training data.\n",
        "   - Lexicons like SentiWordNet or AFINN provide a pre-defined sentiment score for each word, aiding in sentiment classification.\n",
        "\n",
        "These features collectively provide a rich representation of the text, capturing both syntactic and semantic information relevant to sentiment analysis. By incorporating these diverse features, the machine learning model can effectively learn the underlying patterns in the text data and make accurate predictions about sentiment polarity.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dd74a8-dfc0-46b2-d9da-ca0f83919f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Features:\n",
            "[[1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            " [0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "N-grams Features:\n",
            "[[1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            "  0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 1 0 0 0 1 1 0 0 1 0]\n",
            " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "  1 1 0 0 1 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "POS Tags:\n",
            "[['DT', 'NN', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NN', ',', 'NN', 'MD', 'RB', 'VB', 'RB', '.'], ['DT', 'NN', 'VBD', 'RB', 'CC', 'DT', 'NN', 'VBD', 'JJ', '.'], ['NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'PRP$', 'NN', 'NN', ',', 'RB', 'JJ', '.'], ['DT', 'NN', 'NN', 'VBD', 'JJ', 'CC', 'JJ', ',', 'NN', 'VBD', 'DT', 'JJ', 'NN', '.'], ['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'JJ', 'IN', 'PRP$', 'NN', ',', 'RB', 'IN', 'PRP', '.']]\n",
            "\n",
            "Sentiment Lexicon Features:\n",
            "[0, 0, 1, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "# Download the necessary resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"This restaurant has the best food in town, I can't get enough!\",\n",
        "    \"The delivery was fast and the packaging was excellent.\",\n",
        "    \"I had a terrible experience with their customer service, very unprofessional.\",\n",
        "    \"The hotel room was clean and spacious, I had a comfortable stay.\",\n",
        "    \"The price of the product is too high for its quality, not worth it.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in text_data]\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_texts = [[word for word in tokens if word not in stop_words] for tokens in tokenized_texts]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_texts = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_texts]\n",
        "\n",
        "# Bag-of-Words (BoW) feature extraction\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform([' '.join(tokens) for tokens in lemmatized_texts])\n",
        "\n",
        "# N-grams feature extraction\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(1, 2))\n",
        "ngram_features = vectorizer_ngrams.fit_transform([' '.join(tokens) for tokens in lemmatized_texts])\n",
        "\n",
        "# Word embeddings - This part requires pre-trained word embeddings like Word2Vec or GloVe\n",
        "\n",
        "# Part-of-Speech (POS) tagging\n",
        "pos_tags = [nltk.pos_tag(tokens) for tokens in tokenized_texts]\n",
        "pos_features = []\n",
        "for pos_tag in pos_tags:\n",
        "    pos_features.append([tag for word, tag in pos_tag])\n",
        "\n",
        "# Sentiment Lexicons - We'll use a simple lexicon for demonstration\n",
        "sentiment_lexicon = {\n",
        "    'love': 'positive',\n",
        "    'amazing': 'positive',\n",
        "    'terrible': 'negative',\n",
        "    'disappointed': 'negative',\n",
        "    'fantastic': 'positive',\n",
        "    'awful': 'negative'\n",
        "}\n",
        "lexicon_features = []\n",
        "for tokens in lemmatized_texts:\n",
        "    score = sum([1 if token in sentiment_lexicon else 0 for token in tokens])\n",
        "    lexicon_features.append(score)\n",
        "\n",
        "# Print extracted features\n",
        "print(\"Bag-of-Words Features:\")\n",
        "print(bow_features.toarray())\n",
        "print(\"\\nN-grams Features:\")\n",
        "print(ngram_features.toarray())\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_features)\n",
        "print(\"\\nSentiment Lexicon Features:\")\n",
        "print(lexicon_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a74d02-0350-4df3-9db7-8a5d47014ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: restaurant, Chi-square Score: 4.000000000000001\n",
            "Feature: best, Chi-square Score: 4.000000000000001\n",
            "Feature: food, Chi-square Score: 4.000000000000001\n",
            "Feature: town, Chi-square Score: 4.000000000000001\n",
            "Feature: ca, Chi-square Score: 4.000000000000001\n",
            "Feature: get, Chi-square Score: 4.000000000000001\n",
            "Feature: enough, Chi-square Score: 4.000000000000001\n",
            "Feature: delivery, Chi-square Score: 4.000000000000001\n",
            "Feature: fast, Chi-square Score: 4.000000000000001\n",
            "Feature: packaging, Chi-square Score: 4.000000000000001\n",
            "Feature: excellent, Chi-square Score: 4.000000000000001\n",
            "Feature: terrible, Chi-square Score: 4.000000000000001\n",
            "Feature: experience, Chi-square Score: 4.000000000000001\n",
            "Feature: customer, Chi-square Score: 4.000000000000001\n",
            "Feature: service, Chi-square Score: 4.000000000000001\n",
            "Feature: unprofessional, Chi-square Score: 4.000000000000001\n",
            "Feature: hotel, Chi-square Score: 4.000000000000001\n",
            "Feature: room, Chi-square Score: 4.000000000000001\n",
            "Feature: clean, Chi-square Score: 4.000000000000001\n",
            "Feature: spacious, Chi-square Score: 4.000000000000001\n",
            "Feature: comfortable, Chi-square Score: 4.000000000000001\n",
            "Feature: stay, Chi-square Score: 4.000000000000001\n",
            "Feature: price, Chi-square Score: 4.000000000000001\n",
            "Feature: product, Chi-square Score: 4.000000000000001\n",
            "Feature: high, Chi-square Score: 4.000000000000001\n",
            "Feature: quality, Chi-square Score: 4.000000000000001\n",
            "Feature: worth, Chi-square Score: 4.000000000000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "# Download the necessary resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "text_data = [\n",
        "    \"This restaurant has the best food in town, I can't get enough!\",\n",
        "    \"The delivery was fast and the packaging was excellent.\",\n",
        "    \"I had a terrible experience with their customer service, very unprofessional.\",\n",
        "    \"The hotel room was clean and spacious, I had a comfortable stay.\",\n",
        "    \"The price of the product is too high for its quality, not worth it.\"\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in text_data]\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_texts = [[word for word in tokens if word not in stop_words] for tokens in tokenized_texts]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_texts = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_texts]\n",
        "\n",
        "# Bag-of-Words (BoW) feature extraction\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform([' '.join(tokens) for tokens in lemmatized_texts])\n",
        "\n",
        "\n",
        "# Target labels\n",
        "labels = ['Binary', 'Multiclass', 'Multi-label', 'Hierarchical', 'ordinal']\n",
        "\n",
        "# Perform Chi-square test\n",
        "chi2_scores, _ = chi2(bow_features, labels)\n",
        "\n",
        "## Creating dictionary to map features to their Chi-square scores\n",
        "feature_scores = {feature: score for feature, score in zip(vectorizer_bow.vocabulary_.keys(), chi2_scores)}\n",
        "\n",
        "# Sorting features based on their scores in DO\n",
        "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Printing the sorted features & their Chi-square scores\n",
        "for feature, score in sorted_features:\n",
        "    print(f\"Feature: {feature}, Chi-square Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"This restaurant has the best food in town, I can't get enough!\",\n",
        "    \"The delivery was fast and the packaging was excellent.\",\n",
        "    \"I had a terrible experience with their customer service, very unprofessional.\",\n",
        "    \"The hotel room was clean and spacious, I had a comfortable stay.\",\n",
        "    \"The price of the product is too high for its quality, not worth it.\"\n",
        "]\n",
        "\n",
        "# Sample query\n",
        "query = \"I'm looking for a great product recommendation.\"\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the query\n",
        "query_tokens = tokenizer.encode(query, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "text_tokens = tokenizer(text_data, add_special_tokens=True, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "# Get BERT embeddings for the query\n",
        "with torch.no_grad():\n",
        "    query_outputs = model(input_ids=query_tokens)[0][:, 0, :].squeeze().numpy()  # Use the [CLS] token embedding\n",
        "\n",
        "# Get BERT embeddings for the text data\n",
        "with torch.no_grad():\n",
        "    text_outputs = model(input_ids=text_tokens.input_ids)[0][:, 0, :].squeeze().numpy()  # Use the [CLS] token embedding\n",
        "\n",
        "# Calculate cosine similarity between the query and each text document\n",
        "similarity_scores = cosine_similarity([query_outputs], text_outputs)\n",
        "\n",
        "# Zip text data with similarity scores\n",
        "text_similarity_scores = list(zip(text_data, similarity_scores[0]))\n",
        "\n",
        "# Sort text documents based on similarity scores in descending order\n",
        "sorted_text_similarity_scores = sorted(text_similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted text documents and their similarity scores\n",
        "print(\"Ranked Text Documents Based on Similarity to the Query:\")\n",
        "for text, score in sorted_text_similarity_scores:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Similarity Score: {score}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "\n",
        "Learning experience:\n",
        "I learnt a lot about text analysis, taking into account not only the words but also how people use language and share information.\n",
        "\n",
        "Challenges Encountered:\n",
        "It was difficult to strike the correct balance between employing complicated features and making things easy to grasp. Also, gathering trustworthy data for false news identification proved difficult.\n",
        "\n",
        "Importance to Your Field of Study:\n",
        "This practice is appropriate for learning language and computers (NLP). It demonstrates how we may utilize linguistic subtleties and machine learning to identify and address issues such as detecting false news. It's like combining linguistic knowledge with computer abilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}