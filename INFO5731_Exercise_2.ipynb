{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17251A0404/Abhigna_INFO5731_Spring2024/blob/main/INFO5731_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "Research Question: Examining how social media use affects mental health What is the relationship between young adults' self-reported levels of anxiety and depression and the frequency and duration of their social media usage?\n",
        "\n",
        "Gathering of Data:\n",
        "In order to answer this research question, we must gather information on the subsequent variables:\n",
        "\n",
        "Frequency of social media usage: The quantity of times a person uses social media sites in a given day.\n",
        "Duration of social media use: The average amount of time spent each day on social media sites.\n",
        "Self-reported anxiety levels: Participants rate their own anxiety on a scale of 1 to 10.\n",
        "Self-reported depression levels: Participants rate their own depression on a scale of 1 to 10.\n",
        "Procedures for Gathering Data:\n",
        "\n",
        "Create a questionnaire for a survey: Make a survey questionnaire with questions about the frequency and length of social media use as well as self-reported anxiety and depression levels. Make sure the questions are understandable and unambiguous.\n",
        "Choose your participants: To ensure diversity, select a sample of young adults (age range, e.g., 18-30) from a variety of demographic backgrounds.\n",
        "Obtain permission that is informed: Get each participant's informed consent before beginning any data collection by outlining the goal of the study and the intended use of their information.\n",
        "Conduct the survey: Give the survey questionnaire to the chosen respondents using online resources or face-to-face interviews.\n",
        "Gather feedback: Ask participants to provide answers, making sure they are accurate and comprehensive.\n",
        "Data anonymization: To protect participant privacy, remove any personally identifiable information from the replies that have been gathered.\n",
        "\n",
        "Store the information: Store the gathered data in an organised file format, like a CSV file, with each row denoting a participant and the columns standing for the previously listed factors.\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import csv\n",
        "\n",
        "# Define a function to collect survey data\n",
        "def collect_survey_data(filename, num_samples):\n",
        "    with open(filename, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write header row\n",
        "        writer.writerow([\"Social Media Usage Frequency\", \"Social Media Usage Duration (hours)\", \"Anxiety Level\", \"Depression Level\"])\n",
        "        for _ in range(num_samples):\n",
        "            # Simulate data collection (replace with actual survey implementation)\n",
        "            social_media_freq = input(\"Enter social media usage frequency: \")\n",
        "            social_media_duration = input(\"Enter social media usage duration (hours): \")\n",
        "            anxiety_level = input(\"Enter anxiety level (1-10): \")\n",
        "            depression_level = input(\"Enter depression level (1-10): \")\n",
        "            # Write data to CSV file\n",
        "            writer.writerow([social_media_freq, social_media_duration, anxiety_level, depression_level])\n",
        "        print(f\"{num_samples} samples have been collected and saved to {filename}\")\n",
        "\n",
        "# Collect 1000 samples of survey data\n",
        "collect_survey_data(\"survey_data.csv\", 1000)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96143a5-57cb-4215-dd85-d859c9c3ec39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter social media usage frequency: 1\n",
            "Enter social media usage duration (hours): 3\n",
            "Enter anxiety level (1-10): 4\n",
            "Enter depression level (1-10): 9\n",
            "Enter social media usage frequency: 9\n",
            "Enter social media usage duration (hours): 5\n",
            "Enter anxiety level (1-10): 3\n",
            "Enter depression level (1-10): 9\n",
            "2 samples have been collected and saved to survey_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0593461-42fe-4807-a5c4-3162b03d7aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            "Title of the article: The XYZ states revisited\n",
            "Venue/journal/conference being published: International Journal of Modern Physics A\n",
            "Year: 2018\n",
            "Authors: CZ Yuan\n",
            "Abstract: The BESIII and the LHCb became the leading experiments in the study of the exotic states \n",
            "after the Belle and BaBar experiments finished their data taking in the first decade of this …\n",
            "\n",
            "\n",
            "Article 2:\n",
            "Title of the article: An overview of XYZ new particles\n",
            "Venue/journal/conference being published: Chinese Science Bulletin\n",
            "Year: 2014\n",
            "Authors: X Liu\n",
            "Abstract: … (XYZ\\) have been announced by experiments after analyzing various processes. Until now, \n",
            "the family of \\(XYZ… In general, the observed \\(XYZ\\) states can be categorized into five groups, …\n",
            "\n",
            "\n",
            "Article 3:\n",
            "Title of the article: The XYZ states: experimental and theoretical status and perspectives\n",
            "Venue/journal/conference being published: Physics Reports\n",
            "Year: 2020\n",
            "Authors: N Brambilla, S Eidelman, C Hanhart, A Nefediev…\n",
            "Abstract: The quark model was formulated in 1964 to classify mesons as bound states made of a \n",
            "quark–antiquark pair, and baryons as bound states made of three quarks. For a long time all …\n",
            "\n",
            "\n",
            "Article 4:\n",
            "Title of the article: Aplikasi e-commerce penjualan souvenir pernikahan pada toko “XYZ”\n",
            "Venue/journal/conference being published: … : Jurnal Pengembangan Riset …\n",
            "Year: 2017\n",
            "Authors: E Haerulah, S Ismiyatih\n",
            "Abstract: … “XYZ” merupakan salah satu tempat yang menyediakan berbagai macam souvenir … “XYZ” \n",
            "memiliki beberapa macam jenis souvenir dan bermacam-macam harga. Sejauh ini, “XYZ” …\n",
            "\n",
            "\n",
            "Article 5:\n",
            "Title of the article: The ABC-XYZ analysis modified for data with outliers\n",
            "Venue/journal/conference being published: 2018 4th International Conference on …\n",
            "Year: 2018\n",
            "Authors: Z Zenkova, T Kabanova\n",
            "Abstract: The ABC-XYZ analysis is widespread in modern business, especially in the inventory control… \n",
            "for the XYZ-part. The paper proposes a modification of the ABC-XYZ analytical technique for …\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def get_articles_from_google_scholar(keyword, num_of_articles, url):\n",
        "    final_articles_list = []\n",
        "    params = {\n",
        "        \"q\": keyword,\n",
        "        \"as_ylo\": 2014,\n",
        "        \"as_yhi\": 2024,\n",
        "        \"hl\": \"en\",\n",
        "        \"num\": 10  # Number of results per page\n",
        "    }\n",
        "    while len(final_articles_list) < num_of_articles:\n",
        "        response = requests.get(url, params=params)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        results = soup.find_all('div', class_='gs_ri')\n",
        "        for result in results:\n",
        "            article = {}\n",
        "            article['title'] = result.find('h3', class_='gs_rt').text.strip()\n",
        "            article['year'] = result.find('div', class_='gs_a').text.strip().split('-')[1].split(', ')[-1].strip()\n",
        "            data = result.find('div', class_='gs_a').text.strip().split('-')[1]\n",
        "            article['venue'] = data.split(', ')[0].strip() if \", \" in data else None\n",
        "            article['authors'] = result.find('div', class_='gs_a').text.strip().split('-')[0].strip()\n",
        "            article['abstract'] = result.find('div', class_='gs_rs').text.strip()\n",
        "\n",
        "            final_articles_list.append(article)\n",
        "            if len(final_articles_list) >= num_of_articles:\n",
        "                break\n",
        "        next_link = soup.find('a', class_='gs_ico gs_ico_nav_next')\n",
        "        if next_link:\n",
        "            params['start'] = params.get('start', 0) + 10\n",
        "            time.sleep(2)  # Add a delay to avoid overwhelming the server\n",
        "        else:\n",
        "            break\n",
        "    return final_articles_list\n",
        "\n",
        "keyword = \"XYZ\"\n",
        "url = \"https://scholar.google.com/scholar\"\n",
        "num_of_articles = 1000\n",
        "articles = get_articles_from_google_scholar(keyword, num_of_articles, url)\n",
        "\n",
        "# Print the first 5 articles\n",
        "for i in range(5):\n",
        "    print(f\"Article {i+1}:\")\n",
        "    print(\"Title of the article:\", articles[i]['title'])\n",
        "    print(\"Venue/journal/conference being published:\", articles[i]['venue'])\n",
        "    print(\"Year:\", articles[i]['year'])\n",
        "    print(\"Authors:\", articles[i]['authors'])\n",
        "    print(\"Abstract:\", articles[i]['abstract'])\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "20c900df-8070-4ed4-e2c7-010e66130811"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TweepyException",
          "evalue": "Expected token_type to equal \"bearer\", but got None instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepyException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-a8dbb37a9ad5>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Authenticate to Twitter API without credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/auth.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, consumer_key, consumer_secret)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/auth.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, consumer_key, consumer_secret)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_type'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'bearer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             raise TweepyException('Expected token_type to equal \"bearer\", '\n\u001b[0m\u001b[1;32m    149\u001b[0m                                   f'but got {data.get(\"token_type\")} instead')\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepyException\u001b[0m: Expected token_type to equal \"bearer\", but got None instead"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "# Twitter API credentials (leave these blank)\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''\n",
        "\n",
        "# Authenticate to Twitter API without credentials\n",
        "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Check if authentication was successful\n",
        "if not api:\n",
        "    print(\"Authentication failed. Please check your API credentials.\")\n",
        "    exit()\n",
        "\n",
        "# Define a function to fetch tweets based on a hashtag\n",
        "def fetch_tweets_by_hashtag(hashtag, num_tweets):\n",
        "    tweets = []\n",
        "    for tweet in tweepy.Cursor(api.search_tweets, q=hashtag, tweet_mode='extended').items(num_tweets):\n",
        "        tweets.append({\n",
        "            'id': tweet.id_str,\n",
        "            'text': tweet.full_text,\n",
        "            'created_at': tweet.created_at,\n",
        "            'user': {\n",
        "                'id': tweet.user.id_str,\n",
        "                'screen_name': tweet.user.screen_name,\n",
        "                'name': tweet.user.name,\n",
        "            }\n",
        "        })\n",
        "    return tweets\n",
        "\n",
        "# Example usage:\n",
        "hashtag = '#python'\n",
        "num_tweets = 100\n",
        "tweets = fetch_tweets_by_hashtag(hashtag, num_tweets)\n",
        "\n",
        "# Print the first 5 tweets\n",
        "for i, tweet in enumerate(tweets[:5], 1):\n",
        "    print(f\"Tweet {i}:\")\n",
        "    print(\"ID:\", tweet['id'])\n",
        "    print(\"Text:\", tweet['text'])\n",
        "    print(\"Created At:\", tweet['created_at'])\n",
        "    print(\"User ID:\", tweet['user']['id'])\n",
        "    print(\"User Screen Name:\", tweet['user']['screen_name'])\n",
        "    print(\"User Name:\", tweet['user']['name'])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Web scraping involves extracting data from websites using tools like  Scrapy in Python. Understanding HTML structure, DOM, and CSS selectors is crucial.\n",
        "Understanding HTML/CSS, using XPath/CSS selectors, processing HTTP requests, adhering to site policies, handling dynamic content,  utilizing parsing libraries, using regular expressions, and handling data storage,  are followed are some of the key concepts and techniques for web scraping. Maintaining up to date with evolving technology and website updates requires constant learning and flexibility.\n",
        "Challenges associated with web scraping include dynamic content, anti-scraping measures, evolving structures, rate limitation, big datasets, legal issues, session management, a variety of HTML formats, redirects, and compliance with Robots.txt. Headless browsers, IP rotation, adaptive coding, rate limitation, efficiency optimization, and moral behavior are some of the solutions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E9RqrlwdTfvl",
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}